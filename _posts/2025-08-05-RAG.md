
## Why RAG:
Sometimes the information that the model needs to reply to a prompt is recent and without that, the response is going to be wrong. 
Sometimes the information is private such as email or internal documents of a company and so on ... therefore LLM has not seen the data before and when we ask a question about that, it is not going to give us the correct answer.
Therefore we use RAG to provide the new information for the model so that it can respond more specifically and correctly.  

Here is a picture showing how RAG works compared to normal LLM use:

<img width="1053" height="581" alt="image" src="https://github.com/user-attachments/assets/936a7a51-7814-4e49-aa05-7452974a7cf4" />


The user experience is the same as chatting with an LLM model. However this side route helps the LLM to find the relevant and accurate information. 

Now lets see how RAG retrive the information from knowledge base. When a user ask a question, the retriver should look for the relevant document and after finding the relevant docs, a new prompt is generated with the relevant data included and then the new prompt is given to LLM to respond. However searching for a relevant documents is not an easy task. There 2 serach approaches:


**1- Keyword search:** in which the retriver look for the exact words from the prompt in the knowledge base. 
**2- Semantic search:** The retriver looks for the documents that have similar meaning to the prompt. 

in any search method that was mentioned above, the retriver will return 20 to 50 relevant documents, however this documents have a different ranking in each search method. After that the model will use a metadat filtering and only allows some of the relevant documents to be passed on to the LLM. The reason for filtering is that sometimes the person who is prompting is from an specific department and they dont need to receive docs from other departments in a company. 
After filtering all the passed documents coming from both search methods are going to be combined and then ranked again. Then only the documents that bave been ranked highly withh be passed to the LLM.


Here is a picture showing that:
<img width="1073" height="576" alt="image" src="https://github.com/user-attachments/assets/15a982b0-4d03-4636-8e79-e0243a313f00" />

When both Keyword and Semantic serach is used at the same time, the search method is called Hybrid search.

## Metadata filtering:
Let's assume the documents we have in the knowledge base, are news articles, each doument is going to have multiple tags, such as date, published at, url, author, main article, title and so on. In this case, based on one or multiple metadata, we can filter the documents and find what we need easily. In RAG, the retrival is NOT done by metadata filtering. However the documents are being filtered not using the information from the prompt BUT using the information based on the user demographic or other user info. For example if the user is not a paied user, and is prompting to receive info, first all the paid documents are filtered because the user is not a paid user and there he/she can not receive info from the paid articles. 

Metadata filetring alone is not very good for retriving documents and it needs to be paired with other techniques in order to be more effective. 

## Keyword search methods:

### TF-IDF
In this search technique each document is treated like a bag of words. meaning that the order of words are not important. I simply looks at the docs and each docs what have more words like the prompt, it is relevant. when a prompt is sent to the retriver, a sparse vector is made from the prompt and all the documents and then we are ready to start ranking and scoring each doc. each word from the prompt is considered as a keyword, then each document is looks and is given a +1 score for every keyword is conatined/ look athe picture below. 

<img width="1062" height="593" alt="image" src="https://github.com/user-attachments/assets/76e567ec-3a79-4d97-a1f8-ae5f526be7c1" />

if we score each document once for each keyword it contains, we will miss the frequency. If we want to include the frequency in our scoring, then it is called TF scoring, look at the picture below: 
<img width="1011" height="499" alt="image" src="https://github.com/user-attachments/assets/937e0b5e-4c8a-485a-b676-31d0f56f9792" />
 and we need to normalize the score by the length of the document too:
<img width="988" height="514" alt="image" src="https://github.com/user-attachments/assets/658a6937-c9fe-4148-913c-097e04a57b9b" />
But there is still another probelm, rare words which are important might apear less frequent but they are important. to correct for this we do IDF (inverse document frequency):
<img width="1046" height="593" alt="image" src="https://github.com/user-attachments/assets/c071227c-8d26-4775-ad62-7a7d30174ed7" />
So now instead of Term frequency (TF), we use TF-IDF:
<img width="1058" height="575" alt="image" src="https://github.com/user-attachments/assets/243f1c33-a7de-4791-a06a-269a1b7472b4" />

However modern approaches are use BM25 approach instead of TF-IDF appraoch which is a refined version of TF-IDF. Let see what is that:

### BM25 (Best Matching 25):
- In TF-IDF method, if the keyword is found repeatedly in a document, then that document is more important than other documents however the amount of being important is the same as the frequency. for example if the word is found 20 times in a doc and 10 time in another doc, then the first one is twice more important than the second one. In BM25, this is not the case, they first one is more important but not twice ... they have something called term frequency saturation. 
- in TF-IDF method, the longer documents are normalized too aggresively, in BM25 they modify that too

- <img width="1039" height="568" alt="image" src="https://github.com/user-attachments/assets/08e92080-9575-40ea-a9ba-fd8911feb5b1" />

BM25 is the most commonly used search algorithm that has been used for decades.

Now lets talk about the cons of keyword search. In the keyword search if two words are synonyms like "happy" and "glad", they will be ignored becasue keyword search looks for the exact keyword not the synonyms. or python the programing language and python the snake are going to be considered the same words. This is the reason why there is another method for searching the relevant doumnt called "Semantic search". 

## Semantic Search:
Semantic and keyword search from the high level are working the same way. The prompt and the documents will be converted to vectors and then vectors are going to be compared in order to assign scores to each document. But the way that vectors are created are different. There are many embedding models for words, sentences or a whole document. In the embedding space, similar concept tends to be closer to each other. for example: 
<img width="1055" height="586" alt="image" src="https://github.com/user-attachments/assets/0a1a2a5f-50ac-4b54-866c-2b831d29e4d4" />

There are different ways of finding the similarty of two point in the embedding space. There are eucleadian, cosine, dot product and so on ... then you can find the most relevant docs by just finding the distance between vectors.
But how these embedding are really created. For example two words that have similar meaning, such as "Hello" and "Good morning", are positive pairs and two words that have dis-simialr meaning such as good and trumpoline are negative examples and they need to be far from each other. One way of create an embedding is to have lots and lots of positive and negative examples of positice and negative pairs.

First all the vecotrs of all the words are randomly assigned, then based on the loss function, two vectors are compared if they are being far apart, then the interanl parameters going to be updated to pur these two pairs closer to each other. This is call contrastive training. This process is repeated many times, until similar words are put close to each other and the model can not be improved further. 

In the context of RAG, vector embeddings are used for:
Capturing Meaning: Vector embeddings act like a map for text. They convert words and sentences into positions in vector space that capture meaning. These vectors can then be used to locate information matching a query.
Comparing Similarity: When a prompt is received, it is converted into an embedding vector of its own. Then, the similarity between this prompt's vector and other vectors in the database can be calculated. This helps identify texts closest in meaning to the prompt.

<img width="914" height="348" alt="image" src="https://github.com/user-attachments/assets/9af5cad2-9535-4bcc-b048-c8875329aabd" />


One thing is that when embedding the documents, then we need to make sure the token window is alrge enough otherwise after ceratin amount of text, the rest of the text is not being read by the mdoel to generate the embedding. This is an issue that we are going to talk about in a bit. 
In the hybrid search, when the ranked lists of document are provided using keyword or semantic search, the final lists need to be fused together to generate a final ranked list. One method to fuse and rerank these lists is called Reiprocal ranked fusion (RRF). : 
<img width="1033" height="583" alt="image" src="https://github.com/user-attachments/assets/b418678e-c524-4b7d-8a67-83abad5b83bd" />

You can also wieght the fusion to have a more weight for semnatic list than the exact keyword search final list.

## Evaluating retrival:
In order to evaluate a retrival, you will need 3 things: 1- The prompt, 2- the retrived documents, 3- the ground truth (all the relevant docs that your retriver should return).
Two commonly used metrics to evaluate retrival are: Precision and Recall
**Precision**: Total number of relevant retrived docs / total number of retrived docs
**Recall:** Total number of relevant retrived docs/ total number of relevant docs

<img width="1059" height="587" alt="image" src="https://github.com/user-attachments/assets/ab35f920-cbba-4aea-8299-24337223dd92" />
When Recall is equal to 1, it means that all the relevant docs are retrived, when the precision is equal to 1 it means that all the retrived docs are the correct one, the more retrived we have that are incorrect, the precision will go down. but as long as recall is equal to one, it means that all relebant docs are among the retrived ones.
There are several other metrics to evaluate the retrival such as MAP and mean reciprocal rank which I will not expalin here but the goal of all of these metrics is to score our retrivaal.


okay so far then we learned about the Retrival methods and metrics for evaluating them. lets recap again, in the keyword search method, the query become tokenized as well as the knowledge base, then relevant documents will be retrived based on searching the keyword tokens. documents with more keyword in them will be more relevant in this search.longer documents will be normalized ususally using bm25 technique. 
In semantic search however, we usually will use an embedding model and its embedding space. First we use the model encoder to create embeddings from the query. we can encode the douments in the same space. Then we will find the similarity or the distance between the query vector and the documents. Then we find its closets vectors. 
In the hybrid method, then the list of relevant docs using semantic search and keyword search will be combined with Reciprocal ranking fusion technique. 


When searching into very large databases with milion or bilion documents in them, the retrival searhes become super slow. so what shoudl be done in this case? in the semantic search that we have vector space and we created the embedding point for the knowledge and the prompt, for every search we need to calculate the distance between the prompt vector and ALL the document vectors. This is time consuming. here is a picture of what is happening : 
<img width="1050" height="580" alt="image" src="https://github.com/user-attachments/assets/f4c4885c-c642-46af-9564-cf25cbcc4501" />

so instead of KNN, K nearest neighbor search, there are other methods in which the accuracy of retrival will be scarified a little bit but they are faster. These methods are called ANN, Approximate neigherst neighbors. one of the ANN methods is the following:

**Navigable small world** in this method a proximity graph is build first between all the points in the knowledge base embedding space, there are edges between each point and its neighest neighbors. Then query is embedded, a RANDOM node from the knowledge base is chosen, then the distance between that point and query is calculated and then the distance between the doc node's NN and the query point is calculated, if one the neighbors has smaller distance we move to that point and this process continues until no closer neighbor is found. Then another RANDOM point will be chosen and this continues until few neigherst NN to the query are found. This is not garanteed to find the actual nearest neighbors of the query point but is very fast and the results are acceptable.


<img width="1061" height="580" alt="image" src="https://github.com/user-attachments/assets/43003789-dcd5-495f-a883-6908cd344d8b" />

**Hierarchical navigable small world** In this method the way that the proximity graph is build is different: 
<img width="1027" height="589" alt="image" src="https://github.com/user-attachments/assets/6b20009a-6c66-405a-87e5-a9e5f8c98f07" />

Also the way the search is done:
<img width="1066" height="590" alt="image" src="https://github.com/user-attachments/assets/6df1f7d6-74b1-438f-85da-b37090a82d48" />

in the production RAG systems, the vector databases are used which are optimized and have APIs to perform the retrival searches much faster and optimized. One of these vector databases called Weaviate. You can perform Bm25 serach, Semantic serahc, filtering and hyrbid search using this vector database. 
In any RAG system, few more adaptiations are required for the model to perform at scale. In the following, we will introduce these few adaptations: 

## Chunking: 
Imagine that your knowledge base has 1000 books. When you vectorize the knowledge base, each book going to be represented by a vector. but the specific infor of a chapter or a paragraph is going to be lost becasue the entire book is shown with a single vector and a lot of details are lost. instead the vector is an average representation of the book. Therefore the retrival is not going to be as good too for the obvious reasons.
So instead we can chunk the books to pages, paragraphs or even sentences. In this case instead of 1000 books in our knowledge base, we will have millions of paragraphs for example but we dont care becasue the vector base can search through all that easily. for the chunk size of for example 250 characters, it means that from character 1 to 250 are going to be chunk 1, from 250 to 500 are going to be chunk 2 and so on. However since the sentences might be cut in the middle or the chunk might happen in the middle of the word and so on, to fix this, we can allow overlap between the chunks. for example an overlap of 25 character between the chunks. 

The chunch size can be fixed or variant. 
To recap, we chunk the docs, then our vectorbase, vectorize the chunks and then perform all sorts of searching and give us back few relevant documents that are going to be added to the prompt to be given to the LLM to generate the final response. 



