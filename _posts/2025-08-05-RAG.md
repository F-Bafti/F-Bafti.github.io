
## Why RAG:
Sometimes the information that the model needs to reply to a prompt is recent and without that, the response is going to be wrong. 
Sometimes the information is private such as email or internal documents of a company and so on ... therefore LLM has not seen the data before and when we ask a question about that, it is not going to give us the correct answer.
Therefore we use RAG to provide the new information for the model so that it can respond more specifically and correctly.  

Here is a picture showing how RAG works compared to normal LLM use:

<img width="1053" height="581" alt="image" src="https://github.com/user-attachments/assets/936a7a51-7814-4e49-aa05-7452974a7cf4" />


The user experience is the same as chatting with an LLM model. However this side route helps the LLM to find the relevant and accurate information. 

Now lets see how RAG retrive the information from knowledge base. When a user ask a question, the retriver should look for the relevant document and after finding the relevant docs, a new prompt is generated with the relevant data included and then the new prompt is given to LLM to respond. However searching for a relevant documents is not an easy task. There 2 serach approaches:


**1- Keyword search:** in which the retriver look for the exact words from the prompt in the knowledge base. 
**2- Semantic search:** The retriver looks for the documents that have similar meaning to the prompt. 

in any search method that was mentioned above, the retriver will return 20 to 50 relevant documents, however this documents have a different ranking in each search method. After that the model will use a metadat filtering and only allows some of the relevant documents to be passed on to the LLM. The reason for filtering is that sometimes the person who is prompting is from an specific department and they dont need to receive docs from other departments in a company. 
After filtering all the passed documents coming from both search methods are going to be combined and then ranked again. Then only the documents that bave been ranked highly withh be passed to the LLM.


Here is a picture showing that:
<img width="1073" height="576" alt="image" src="https://github.com/user-attachments/assets/15a982b0-4d03-4636-8e79-e0243a313f00" />

When both Keyword and Semantic serach is used at the same time, the search method is called Hybrid search.

## Metadata filtering:
Let's assume the documents we have in the knowledge base, are news articles, each doument is going to have multiple tags, such as date, published at, url, author, main article, title and so on. In this case, based on one or multiple metadata, we can filter the documents and find what we need easily. In RAG, the retrival is NOT done by metadata filtering. However the documents are being filtered not using the information from the prompt BUT using the information based on the user demographic or other user info. For example if the user is not a paied user, and is prompting to receive info, first all the paid documents are filtered because the user is not a paid user and there he/she can not receive info from the paid articles. 

Metadata filetring alone is not very good for retriving documents and it needs to be paired with other techniques in order to be more effective. 

## Keyword search methods:

### TF-IDF
In this search technique each document is treated like a bag of words. meaning that the order of words are not important. I simply looks at the docs and each docs what have more words like the prompt, it is relevant. when a prompt is sent to the retriver, a sparse vector is made from the prompt and all the documents and then we are ready to start ranking and scoring each doc. each word from the prompt is considered as a keyword, then each document is looks and is given a +1 score for every keyword is conatined/ look athe picture below. 

<img width="1062" height="593" alt="image" src="https://github.com/user-attachments/assets/76e567ec-3a79-4d97-a1f8-ae5f526be7c1" />

if we score each document once for each keyword it contains, we will miss the frequency. If we want to include the frequency in our scoring, then it is called TF scoring, look at the picture below: 
<img width="1011" height="499" alt="image" src="https://github.com/user-attachments/assets/937e0b5e-4c8a-485a-b676-31d0f56f9792" />
 and we need to normalize the score by the length of the document too:
<img width="988" height="514" alt="image" src="https://github.com/user-attachments/assets/658a6937-c9fe-4148-913c-097e04a57b9b" />
But there is still another probelm, rare words which are important might apear less frequent but they are important. to correct for this we do IDF (inverse document frequency):
<img width="1046" height="593" alt="image" src="https://github.com/user-attachments/assets/c071227c-8d26-4775-ad62-7a7d30174ed7" />
So now instead of Term frequency (TF), we use TF-IDF:
<img width="1058" height="575" alt="image" src="https://github.com/user-attachments/assets/243f1c33-a7de-4791-a06a-269a1b7472b4" />

However modern approaches are use BM25 approach instead of TF-IDF appraoch which is a refined version of TF-IDF. Let see what is that:

### BM25 (Best Matching 25):
- In TF-IDF method, if the keyword is found repeatedly in a document, then that document is more important than other documents however the amount of being important is the same as the frequency. for example if the word is found 20 times in a doc and 10 time in another doc, then the first one is twice more important than the second one. In BM25, this is not the case, they first one is more important but not twice ... they have something called term frequency saturation. 
- in TF-IDF method, the longer documents are normalized too aggresively, in BM25 they modify that too

- <img width="1039" height="568" alt="image" src="https://github.com/user-attachments/assets/08e92080-9575-40ea-a9ba-fd8911feb5b1" />

BM25 is the most commonly used search algorithm that has been used for decades.

Now lets talk about the cons of keyword search. In the keyword search if two words are synonyms like "happy" and "glad", they will be ignored becasue keyword search looks for the exact keyword not the synonyms. or python the programing language and python the snake are going to be considered the same words. This is the reason why there is another method for searching the relevant doumnt called "Semantic search". 

## Semantic Search:
Semantic and keyword search from the high level are working the same way. The prompt and the documents will be converted to vectors and then vectors are going to be compared in order to assign scores to each document. But the way that vectors are created are different. There are many embedding models for words, sentences or a whole document. In the embedding space, similar concept tends to be closer to each other.

